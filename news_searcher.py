import asyncio
import aiohttp
import feedparser
import logging
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
# from newspaper import Article  # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º —Å lxml.html.clean
from config import RSS_SOURCES, MAX_NEWS_ARTICLES, NEWS_SEARCH_TIMEOUT, ENABLE_NEWS_SEARCH

logger = logging.getLogger(__name__)

class NewsSearcher:
    def __init__(self):
        self.session = None
        self.rss_sources = RSS_SOURCES
        self.max_articles = MAX_NEWS_ARTICLES
        self.timeout = NEWS_SEARCH_TIMEOUT
        self.enabled = ENABLE_NEWS_SEARCH
    
    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=self.timeout)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()
    
    async def search_news_by_topic(self, topic: str, max_results: int = None) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–µ–º–µ"""
        if not self.enabled:
            logger.info("News search is disabled")
            return []
        
        max_results = max_results or self.max_articles
        logger.info(f"Searching news for topic: {topic}")
        
        try:
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            all_articles = []
            
            # 1. –ü–æ–∏—Å–∫ –≤ RSS –ª–µ–Ω—Ç–∞—Ö
            rss_articles = await self._search_rss_feeds(topic, max_results // 2)
            all_articles.extend(rss_articles)
            
            # 2. –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google News (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            google_articles = await self._search_google_news(topic, max_results // 2)
            all_articles.extend(google_articles)
            
            # 3. –ü–æ–∏—Å–∫ –≤ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç—è—Ö
            yandex_articles = await self._search_yandex_news(topic, max_results // 2)
            all_articles.extend(yandex_articles)
            
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
            unique_articles = self._remove_duplicates(all_articles)
            sorted_articles = sorted(
                unique_articles, 
                key=lambda x: x.get('published', datetime.min), 
                reverse=True
            )
            
            return sorted_articles[:max_results]
            
        except Exception as e:
            logger.error(f"Error searching news: {e}")
            return []
    
    async def get_latest_news(self, max_results: int = None) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        max_results = max_results or self.max_articles
        logger.info("Fetching latest news")
        
        try:
            all_articles = []
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS –ª–µ–Ω—Ç
            for source in self.rss_sources:
                try:
                    articles = await self._fetch_rss_feed(source, max_results // len(self.rss_sources))
                    all_articles.extend(articles)
                except Exception as e:
                    logger.warning(f"Error fetching from {source}: {e}")
                    continue
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø
            sorted_articles = sorted(
                all_articles,
                key=lambda x: x.get('published', datetime.min),
                reverse=True
            )
            
            return sorted_articles[:max_results]
            
        except Exception as e:
            logger.error(f"Error getting latest news: {e}")
            return []
    
    async def _search_rss_feeds(self, topic: str, max_results: int) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –≤ RSS –ª–µ–Ω—Ç–∞—Ö"""
        articles = []
        topic_lower = topic.lower()
        
        for source in self.rss_sources:
            try:
                feed_articles = await self._fetch_rss_feed(source)
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–µ–º–µ
                for article in feed_articles:
                    title = article.get('title', '').lower()
                    summary = article.get('summary', '').lower()
                    
                    if topic_lower in title or topic_lower in summary:
                        articles.append(article)
                        
                        if len(articles) >= max_results:
                            break
                
                if len(articles) >= max_results:
                    break
                    
            except Exception as e:
                logger.warning(f"Error searching RSS feed {source}: {e}")
                continue
        
        return articles
    
    async def _fetch_rss_feed(self, url: str, max_articles: int = 20) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –∏–∑ RSS –ª–µ–Ω—Ç—ã"""
        try:
            if self.session:
                async with self.session.get(url) as response:
                    content = await response.text()
            else:
                # Fallback –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                response = requests.get(url, timeout=self.timeout)
                content = response.text
            
            feed = feedparser.parse(content)
            articles = []
            
            for entry in feed.entries[:max_articles]:
                article = {
                    'title': entry.get('title', 'No title'),
                    'summary': entry.get('summary', entry.get('description', '')),
                    'link': entry.get('link', ''),
                    'published': self._parse_date(entry.get('published')),
                    'source': feed.feed.get('title', url),
                    'type': 'rss'
                }
                articles.append(article)
            
            return articles
            
        except Exception as e:
            logger.error(f"Error fetching RSS feed {url}: {e}")
            return []
    
    async def _search_google_news(self, topic: str, max_results: int) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –≤ Google News (—á–µ—Ä–µ–∑ RSS)"""
        try:
            # Google News RSS URL
            google_news_url = f"https://news.google.com/rss/search?q={topic}&hl=ru&gl=RU&ceid=RU:ru"
            
            return await self._fetch_rss_feed(google_news_url, max_results)
            
        except Exception as e:
            logger.error(f"Error searching Google News: {e}")
            return []
    
    async def _search_yandex_news(self, topic: str, max_results: int) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –≤ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç—è—Ö"""
        try:
            # –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–∏ RSS (–æ–±—â–∞—è –ª–µ–Ω—Ç–∞)
            yandex_url = "https://news.yandex.ru/index.rss"
            articles = await self._fetch_rss_feed(yandex_url, max_results * 2)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–µ–º–µ
            topic_lower = topic.lower()
            filtered_articles = []
            
            for article in articles:
                title = article.get('title', '').lower()
                summary = article.get('summary', '').lower()
                
                if topic_lower in title or topic_lower in summary:
                    filtered_articles.append(article)
                    
                    if len(filtered_articles) >= max_results:
                        break
            
            return filtered_articles
            
        except Exception as e:
            logger.error(f"Error searching Yandex News: {e}")
            return []
    
    def _parse_date(self, date_str: str) -> datetime:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
        if not date_str:
            return datetime.min
        
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
            formats = [
                '%a, %d %b %Y %H:%M:%S %z',
                '%a, %d %b %Y %H:%M:%S %Z',
                '%Y-%m-%dT%H:%M:%S%z',
                '%Y-%m-%d %H:%M:%S',
                '%d.%m.%Y %H:%M'
            ]
            
            for fmt in formats:
                try:
                    return datetime.strptime(date_str, fmt)
                except ValueError:
                    continue
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥–æ—à–ª–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è
            return datetime.now()
            
        except Exception:
            return datetime.min
    
    def _remove_duplicates(self, articles: List[Dict]) -> List[Dict]:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ç–∞—Ç–µ–π"""
        seen_titles = set()
        unique_articles = []
        
        for article in articles:
            title = article.get('title', '').strip().lower()
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_articles.append(article)
        
        return unique_articles
    
    async def get_article_content(self, url: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç–∞—Ç—å–∏ (–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ)"""
        try:
            # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º —Å newspaper3k –∏ lxml.html.clean
            # –í –±—É–¥—É—â–∏—Ö –≤–µ—Ä—Å–∏—è—Ö –±—É–¥–µ—Ç –∑–∞–º–µ–Ω–µ–Ω–æ –Ω–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
            logger.warning("Article content extraction temporarily disabled due to lxml.html.clean compatibility issues")
            return None

            # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–¥ (–∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω):
            # article = Article(url)
            # article.download()
            # article.parse()
            # return article.text

        except Exception as e:
            logger.error(f"Error getting article content from {url}: {e}")
            return None
    
    def format_news_summary(self, articles: List[Dict], max_articles: int = 5) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é"""
        if not articles:
            return "üì∞ –ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
        
        summary = "üì∞ **–ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:**\n\n"
        
        for i, article in enumerate(articles[:max_articles], 1):
            title = article.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')
            source = article.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫')
            link = article.get('link', '')
            published = article.get('published')
            
            summary += f"**{i}. {title}**\n"
            summary += f"üìÖ {source}"
            
            if published and published != datetime.min:
                summary += f" ‚Ä¢ {published.strftime('%d.%m.%Y %H:%M')}"
            
            if link:
                summary += f"\nüîó [–ß–∏—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é]({link})"
            
            summary += "\n\n"
        
        return summary

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–µ–∑ async context manager
async def search_news(topic: str, max_results: int = 5) -> List[Dict]:
    """–ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    async with NewsSearcher() as searcher:
        return await searcher.search_news_by_topic(topic, max_results)

async def get_latest_news(max_results: int = 5) -> List[Dict]:
    """–ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
    async with NewsSearcher() as searcher:
        return await searcher.get_latest_news(max_results)
